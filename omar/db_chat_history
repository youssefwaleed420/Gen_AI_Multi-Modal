import streamlit as st
import psycopg2
import psycopg2.extras
import voyageai
import faiss
import numpy as np
import fitz  # PyMuPDF
from PIL import Image
import io
import requests
from neo4j import GraphDatabase
from typing import List, Dict, Union, Optional, Tuple
import os
import json
import re
from datetime import datetime, timedelta
import logging
import uuid
import hashlib

# Configuration
VOYAGE_API_KEY = "pa-tDh9PAJmIfaPahq1-GkuSk8uVNGrI69sq3uxpiGK8Y7"
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_CHAT_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "llama3.2:1b"
PDF_PATH = r"C:\Users\STW\Downloads\MA_DWM1000_2000_en_120509.pdf"

# Supabase PostgreSQL Connection
DB_CONFIG = {
    "dbname": "postgres",
    "user": "postgres.jsgdmwbhzvwdhardoimb",
    "password": "omernasser123",
    "host": "aws-0-us-west-1.pooler.supabase.com",
    "port": "6543"
}

# Neo4j Connection
NEO4J_URI = "bolt://localhost:7687"
NEO4J_AUTH = ("neo4j", "omarnasser")

# Initialize Voyage AI client
voyageai.api_key = VOYAGE_API_KEY
client = voyageai.Client()

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Streamlit page config
st.set_page_config(
    page_title="GraphRAG Chat System",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

class DatabaseManager:
    """Manages PostgreSQL database operations for chat history"""
    
    def __init__(self):
        self.connection = None
        self.connect()
        self.setup_tables()
    
    def connect(self):
        """Connect to PostgreSQL database"""
        try:
            self.connection = psycopg2.connect(**DB_CONFIG)
            logger.info("‚úÖ Successfully connected to Supabase PostgreSQL!")
        except Exception as e:
            logger.error(f"‚ùå Failed to connect to database: {str(e)}")
            st.error(f"Database connection failed: {str(e)}")
            self.connection = None
    
    def setup_tables(self):
        """Create necessary tables if they don't exist"""
        if not self.connection:
            return
            
        try:
            cursor = self.connection.cursor()
            
            # Create chats table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS chats (
                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    title TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    user_id TEXT DEFAULT 'default_user',
                    is_active BOOLEAN DEFAULT true
                );
            """)
            
            # Create messages table
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS messages (
                    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                    chat_id UUID REFERENCES chats(id) ON DELETE CASCADE,
                    role TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system')),
                    content TEXT NOT NULL,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    metadata JSONB DEFAULT '{}',
                    context_used INTEGER DEFAULT 0,
                    graph_context_used INTEGER DEFAULT 0
                );
            """)
            
            # Create indexes for better performance
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_chats_user_id ON chats(user_id);
                CREATE INDEX IF NOT EXISTS idx_chats_created_at ON chats(created_at DESC);
                CREATE INDEX IF NOT EXISTS idx_messages_chat_id ON messages(chat_id);
                CREATE INDEX IF NOT EXISTS idx_messages_timestamp ON messages(timestamp DESC);
            """)
            
            self.connection.commit()
            logger.info("üõ†Ô∏è Database tables created or already exist.")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to setup tables: {str(e)}")
            if self.connection:
                self.connection.rollback()
    
    def create_chat(self, title: str, user_id: str = "default_user") -> str:
        """Create a new chat and return its ID"""
        if not self.connection:
            return None
            
        try:
            cursor = self.connection.cursor()
            chat_id = str(uuid.uuid4())
            
            cursor.execute("""
                INSERT INTO chats (id, title, user_id)
                VALUES (%s, %s, %s)
                RETURNING id;
            """, (chat_id, title, user_id))
            
            result = cursor.fetchone()
            self.connection.commit()
            
            logger.info(f"‚úÖ Created new chat: {title}")
            return result[0] if result else chat_id
            
        except Exception as e:
            logger.error(f"‚ùå Failed to create chat: {str(e)}")
            if self.connection:
                self.connection.rollback()
            return None
    
    def get_chats(self, user_id: str = "default_user", limit: int = 50) -> List[Dict]:
        """Get all chats for a user"""
        if not self.connection:
            return []
            
        try:
            cursor = self.connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            cursor.execute("""
                SELECT c.*, 
                       COUNT(m.id) as message_count,
                       MAX(m.timestamp) as last_message_time
                FROM chats c
                LEFT JOIN messages m ON c.id = m.chat_id
                WHERE c.user_id = %s AND c.is_active = true
                GROUP BY c.id
                ORDER BY COALESCE(MAX(m.timestamp), c.created_at) DESC
                LIMIT %s;
            """, (user_id, limit))
            
            return cursor.fetchall()
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get chats: {str(e)}")
            return []
    
    def get_chat_messages(self, chat_id: str) -> List[Dict]:
        """Get all messages for a specific chat"""
        if not self.connection:
            return []
            
        try:
            cursor = self.connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            
            cursor.execute("""
                SELECT * FROM messages 
                WHERE chat_id = %s 
                ORDER BY timestamp ASC;
            """, (chat_id,))
            
            return cursor.fetchall()
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get messages: {str(e)}")
            return []
    
    def add_message(self, chat_id: str, role: str, content: str, 
                   context_used: int = 0, graph_context_used: int = 0, 
                   metadata: Dict = None) -> bool:
        """Add a message to a chat"""
        if not self.connection:
            return False
            
        try:
            cursor = self.connection.cursor()
            
            cursor.execute("""
                INSERT INTO messages (chat_id, role, content, context_used, graph_context_used, metadata)
                VALUES (%s, %s, %s, %s, %s, %s);
            """, (chat_id, role, content, context_used, graph_context_used, 
                  json.dumps(metadata or {})))
            
            # Update chat's updated_at timestamp
            cursor.execute("""
                UPDATE chats SET updated_at = CURRENT_TIMESTAMP WHERE id = %s;
            """, (chat_id,))
            
            self.connection.commit()
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to add message: {str(e)}")
            if self.connection:
                self.connection.rollback()
            return False
    
    def delete_chat(self, chat_id: str) -> bool:
        """Delete a chat (soft delete)"""
        if not self.connection:
            return False
            
        try:
            cursor = self.connection.cursor()
            
            cursor.execute("""
                UPDATE chats SET is_active = false WHERE id = %s;
            """, (chat_id,))
            
            self.connection.commit()
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to delete chat: {str(e)}")
            if self.connection:
                self.connection.rollback()
            return False
    
    def update_chat_title(self, chat_id: str, new_title: str) -> bool:
        """Update chat title"""
        if not self.connection:
            return False
            
        try:
            cursor = self.connection.cursor()
            
            cursor.execute("""
                UPDATE chats SET title = %s, updated_at = CURRENT_TIMESTAMP 
                WHERE id = %s;
            """, (new_title, chat_id))
            
            self.connection.commit()
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to update chat title: {str(e)}")
            if self.connection:
                self.connection.rollback()
            return False
    
    def close(self):
        """Close database connection"""
        if self.connection:
            self.connection.close()
            logger.info("üîö Database connection closed.")

class OllamaLLM:
    """Dedicated Ollama LLM handler with improved error handling and features"""
    
    def __init__(self, base_url: str = OLLAMA_URL, model: str = OLLAMA_MODEL):
        self.base_url = base_url
        self.chat_url = OLLAMA_CHAT_URL
        self.model = model
        self.check_ollama_connection()
    
    def check_ollama_connection(self):
        """Check if Ollama is running and model is available"""
        try:
            # Check if Ollama is running
            response = requests.get(f"{self.base_url.replace('/api/generate', '')}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                
                if self.model not in model_names:
                    logger.warning(f"‚ö†Ô∏è Model '{self.model}' not found. Available models: {model_names}")
                    if model_names:
                        self.model = model_names[0]
                        logger.info(f"üîÑ Switching to available model: {self.model}")
                else:
                    logger.info(f"‚úÖ Ollama connected with model: {self.model}")
            else:
                logger.error("‚ùå Ollama server not responding")
        except Exception as e:
            logger.error(f"‚ùå Failed to connect to Ollama: {str(e)}")
            logger.info("üí° Make sure Ollama is running: 'ollama serve'")
    
    def simple_generate(self, prompt: str, **kwargs) -> str:
        """Simple text generation using /api/generate endpoint"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": kwargs.get("temperature", 0.3),
                    "top_p": kwargs.get("top_p", 0.9),
                    "num_ctx": kwargs.get("num_ctx", 2048),
                    "repeat_penalty": kwargs.get("repeat_penalty", 1.1)
                }
            }
            
            response = requests.post(self.base_url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "No response from LLM")
            
        except requests.exceptions.Timeout:
            return "Error: LLM request timed out"
        except requests.exceptions.RequestException as e:
            return f"Error: LLM request failed - {str(e)}"
        except Exception as e:
            return f"Error: {str(e)}"
    
    def chat_generate(self, messages: List[Dict], **kwargs) -> str:
        """Chat-based generation using /api/chat endpoint"""
        try:
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "top_p": kwargs.get("top_p", 0.9),
                    "num_ctx": kwargs.get("num_ctx", 4096),
                    "repeat_penalty": kwargs.get("repeat_penalty", 1.1)
                }
            }
            
            response = requests.post(self.chat_url, json=payload, timeout=90)
            response.raise_for_status()
            
            result = response.json()
            return result.get("message", {}).get("content", "No response from LLM")
            
        except requests.exceptions.Timeout:
            return "Error: Chat request timed out"
        except requests.exceptions.RequestException as e:
            return f"Error: Chat request failed - {str(e)}"
        except Exception as e:
            return f"Error: {str(e)}"

class GraphRAGSystem:
    def __init__(self, db_manager: DatabaseManager):
        self.faiss_index = None
        self.metadata = []
        self.neo4j_driver = None
        self.vector_index_created = False
        self.knowledge_graph_created = False
        self.db_manager = db_manager
        self.llm = OllamaLLM()
        
    def connect_neo4j(self):
        """Connect to Neo4j database and setup GraphRAG schema"""
        try:
            self.neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)
            
            # Verify connection
            with self.neo4j_driver.session() as session:
                result = session.run("RETURN 1 AS test")
                if result.single()["test"] == 1:
                    logger.info("‚úÖ Successfully connected to Neo4j")
                    
            # Create vector index and knowledge graph schema
            self.create_neo4j_vector_index()
            self.create_knowledge_graph_schema()
            
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Neo4j connection failed: {str(e)}")
            self.neo4j_driver = None

    def create_neo4j_vector_index(self):
        """Create vector index in Neo4j"""
        if not self.neo4j_driver:
            return
            
        try:
            with self.neo4j_driver.session() as session:
                # Check if vector index exists
                index_exists = session.run(
                    "SHOW INDEXES WHERE type = 'VECTOR' AND name = 'document_embeddings'"
                ).single()
                
                if not index_exists:
                    session.run("""
                    CREATE VECTOR INDEX document_embeddings 
                    FOR (d:Document) ON (d.embedding)
                    OPTIONS {
                        indexConfig: {
                            `vector.dimensions`: 1024,
                            `vector.similarity_function`: 'cosine'
                        }
                    }
                    """)
                    logger.info("‚úÖ Created vector index in Neo4j")
                    
                self.vector_index_created = True
                    
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to create vector index: {str(e)}")
            self.vector_index_created = False

    def create_knowledge_graph_schema(self):
        """Create GraphRAG knowledge graph schema"""
        if not self.neo4j_driver:
            return
            
        try:
            with self.neo4j_driver.session() as session:
                # Create constraints and indexes for GraphRAG
                session.run("""
                CREATE CONSTRAINT unique_concept IF NOT EXISTS 
                FOR (c:Concept) REQUIRE c.name IS UNIQUE
                """)
                
                session.run("""
                CREATE CONSTRAINT unique_entity IF NOT EXISTS 
                FOR (e:Entity) REQUIRE e.name IS UNIQUE
                """)
                
                # Create fulltext search index
                try:
                    session.run("""
                    CREATE FULLTEXT INDEX conceptSearch IF NOT EXISTS 
                    FOR (n:Concept|Entity) ON EACH [n.name, n.description]
                    """)
                except:
                    pass  # Index might already exist
                
                # Create vector index for concept embeddings
                try:
                    session.run("""
                    CREATE VECTOR INDEX concept_embeddings IF NOT EXISTS
                    FOR (c:Concept) ON (c.embedding)
                    OPTIONS {
                        indexConfig: {
                            `vector.dimensions`: 1024,
                            `vector.similarity_function`: 'cosine'
                        }
                    }
                    """)
                except:
                    pass  # Index might already exist
                
                logger.info("‚úÖ Created GraphRAG knowledge graph schema")
                self.knowledge_graph_created = True
                
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to create knowledge graph schema: {str(e)}")
            self.knowledge_graph_created = False

    def extract_entities_and_concepts(self, text: str) -> Dict:
        """Extract entities and concepts using Ollama LLM"""
        prompt = f"""Extract key concepts, entities, and their relationships from this technical text. 
Focus on technical terms, specifications, product names, and important concepts.

Text: {text[:2000]}...

Return ONLY a valid JSON object with this exact structure:
{{
    "concepts": [
        {{"name": "concept_name", "description": "brief description", "importance": 0.8, "type": "technical"}}
    ],
    "entities": [
        {{"name": "entity_name", "description": "brief description", "type": "product"}}
    ],
    "relationships": [
        {{"source": "source_name", "target": "target_name", "type": "relates_to", "strength": 0.7}}
    ]
}}

Ensure all JSON is properly formatted and valid."""
        
        try:
            response = self.llm.simple_generate(prompt, temperature=0.1, num_ctx=3000)
            
            # Clean the response to extract JSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                parsed_data = json.loads(json_str)
                
                # Ensure required fields exist
                parsed_data.setdefault('concepts', [])
                parsed_data.setdefault('entities', [])
                parsed_data.setdefault('relationships', [])
                
                return parsed_data
            else:
                logger.warning("No valid JSON found in LLM response")
                return {"concepts": [], "entities": [], "relationships": []}
                
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {str(e)}")
            return {"concepts": [], "entities": [], "relationships": []}
        except Exception as e:
            logger.error(f"Failed to extract entities: {str(e)}")
            return {"concepts": [], "entities": [], "relationships": []}

    def store_knowledge_graph(self, knowledge: Dict, source_text: str):
        """Store extracted knowledge in Neo4j graph"""
        if not self.neo4j_driver or not self.knowledge_graph_created:
            return
            
        try:
            with self.neo4j_driver.session() as session:
                # Store concepts with embeddings
                for concept in knowledge.get('concepts', []):
                    try:
                        # Generate embedding for concept
                        concept_text = concept['name'] + " " + concept.get('description', '')
                        concept_embedding = client.multimodal_embed(
                            inputs=[[concept_text]],
                            model="voyage-multimodal-3",
                            input_type="document"
                        ).embeddings[0]
                        
                        session.run("""
                        MERGE (c:Concept {name: $name})
                        SET c.description = $description,
                            c.importance = $importance,
                            c.type = $type,
                            c.embedding = $embedding,
                            c.last_mentioned = datetime(),
                            c.mention_count = coalesce(c.mention_count, 0) + 1
                        """, {
                            **concept,
                            'embedding': concept_embedding
                        })
                    except Exception as e:
                        logger.error(f"Failed to store concept {concept.get('name', 'unknown')}: {str(e)}")
                        continue
                
                # Store entities
                for entity in knowledge.get('entities', []):
                    session.run("""
                    MERGE (e:Entity {name: $name})
                    SET e.description = $description,
                        e.type = $type,
                        e.last_mentioned = datetime(),
                        e.mention_count = coalesce(e.mention_count, 0) + 1
                    """, entity)
                
                # Store relationships
                for rel in knowledge.get('relationships', []):
                    session.run("""
                    MATCH (source) WHERE source.name = $source
                    MATCH (target) WHERE target.name = $target
                    MERGE (source)-[r:RELATES {type: $type}]->(target)
                    SET r.strength = $strength,
                        r.last_used = datetime(),
                        r.usage_count = coalesce(r.usage_count, 0) + 1
                    """, rel)
                
                logger.info(f"‚úÖ Stored knowledge graph: {len(knowledge.get('concepts', []))} concepts, {len(knowledge.get('entities', []))} entities")
                
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to store knowledge graph: {str(e)}")

    def get_graph_context(self, query: str, top_k: int = 5) -> List[Dict]:
        """Retrieve relevant context from knowledge graph"""
        if not self.neo4j_driver or not self.knowledge_graph_created:
            return []
            
        try:
            # Generate query embedding
            query_embedding = client.multimodal_embed(
                inputs=[[query]],
                model="voyage-multimodal-3",
                input_type="document"
            ).embeddings[0]
            
            with self.neo4j_driver.session() as session:
                # Vector similarity search on concepts
                vector_results = []
                try:
                    vector_results = session.run("""
                    CALL db.index.vector.queryNodes('concept_embeddings', $top_k, $embedding)
                    YIELD node, score
                    MATCH (node)-[r:RELATES]-(related)
                    RETURN node.name as concept, 
                           node.description as description,
                           score,
                           collect(DISTINCT {name: related.name, type: labels(related)[0], relationship: r.type}) as related_items
                    ORDER BY score DESC
                    """, {"top_k": top_k, "embedding": query_embedding}).data()
                except Exception as e:
                    logger.warning(f"Vector search failed: {str(e)}")
                
                # Also try fulltext search as fallback
                fulltext_results = []
                try:
                    fulltext_results = session.run("""
                    CALL db.index.fulltext.queryNodes('conceptSearch', $query)
                    YIELD node, score
                    WHERE score > 0.5
                    MATCH (node)-[r:RELATES]-(related)
                    RETURN node.name as concept, 
                           node.description as description,
                           score,
                           collect(DISTINCT {name: related.name, type: labels(related)[0], relationship: r.type}) as related_items
                    ORDER BY score DESC
                    LIMIT $top_k
                    """, {"query": query, "top_k": top_k}).data()
                except Exception as e:
                    logger.warning(f"Fulltext search failed: {str(e)}")
                
                # Combine and deduplicate results
                all_results = vector_results + fulltext_results
                seen_concepts = set()
                unique_results = []
                
                for result in all_results:
                    if result['concept'] not in seen_concepts:
                        seen_concepts.add(result['concept'])
                        unique_results.append(result)
                
                return unique_results[:top_k]
                
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to get graph context: {str(e)}")
            return []

    def process_pdf(self, pdf_path: str) -> List[Union[str, Image.Image]]:
        """Extract text and images from PDF"""
        doc = fitz.open(pdf_path)
        pdf_text = ""
        pdf_images = []

        for page in doc:
            # Extract text
            pdf_text += page.get_text() + "\n"
            
            # Extract images (limit to avoid too many)
            image_list = page.get_images(full=True)
            for i, img in enumerate(image_list):
                if len(pdf_images) >= 5:  # Limit images
                    break
                try:
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    pdf_images.append(Image.open(io.BytesIO(image_bytes)).resize((256, 256)))
                except:
                    continue
        
        doc.close()
        return [pdf_text, *pdf_images] if pdf_images else [pdf_text]

    def chunk_document(self, text: str, chunk_size: int = 1000) -> List[str]:
        """Split document into manageable chunks"""
        words = text.split()
        chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
        return chunks

    def create_faiss_index(self, documents: List[List[Union[str, Image.Image]]]):
        """Create FAISS index and extract knowledge from documents"""
        # Process documents into chunks
        processed_docs = []
        all_text = ""
        
        for doc in documents:
            text = doc[0]
            images = doc[1:] if len(doc) > 1 else []
            all_text += text + " "
            
            # Split text into chunks
            chunks = self.chunk_document(text)
            for chunk in chunks:
                # Only add images to first few chunks to avoid too many
                if len(processed_docs) < 3 and images:
                    processed_docs.append([chunk, *images[:2]])  # Max 2 images per chunk
                else:
                    processed_docs.append([chunk])
        
        logger.info(f"üîÑ Processing {len(processed_docs)} document chunks...")
        
        # Extract and store knowledge from the entire document
        logger.info("üß† Extracting knowledge graph from document...")
        knowledge = self.extract_entities_and_concepts(all_text)
        self.store_knowledge_graph(knowledge, all_text)
        
        # Get embeddings for all chunks
        response = client.multimodal_embed(
            inputs=processed_docs,
            model="voyage-multimodal-3",
            input_type="document"
        )
        
        embeddings = np.array(response.embeddings).astype("float32")
        dimension = embeddings.shape[1]
        
        # Create and populate FAISS index
        self.faiss_index = faiss.IndexFlatL2(dimension)
        self.faiss_index.add(embeddings)
        self.metadata = [doc[0] for doc in processed_docs]
        
        logger.info(f"‚úÖ Created FAISS index with {len(embeddings)} vectors")
        return response, processed_docs

    def store_in_neo4j(self, documents: List[List[Union[str, Image.Image]]], embeddings: List[List[float]]):
        """Store documents and embeddings in Neo4j"""
        if not self.neo4j_driver or not self.vector_index_created:
            logger.warning("‚ö†Ô∏è Neo4j not properly configured - skipping document storage")
            return
            
        try:
            with self.neo4j_driver.session() as session:
                for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
                    session.run(
                        """
                        MERGE (d:Document {id: $id})
                        SET d.text = $text,
                            d.embedding = $embedding,
                            d.has_image = $has_image,
                            d.source = $source,
                            d.chunk_index = $chunk_index,
                            d.timestamp = datetime()
                        """,
                        {
                            "id": f"doc_{i}",
                            "text": doc[0],
                            "embedding": embedding,
                            "has_image": len(doc) > 1,
                            "source": PDF_PATH,
                            "chunk_index": i
                        }
                    )
            logger.info(f"üíæ Stored {len(documents)} document chunks in Neo4j")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to store documents in Neo4j: {str(e)}")

    def graphrag_search(self, query: str, top_k: int = 3) -> Dict:
        """Enhanced search using GraphRAG approach"""
        
        # Extract knowledge from query and store in graph
        query_knowledge = self.extract_entities_and_concepts(query)
        self.store_knowledge_graph(query_knowledge, query)
        
        # Get graph context
        graph_context = self.get_graph_context(query, top_k)
        
        # Enhance query with graph context
        context_terms = []
        for context in graph_context:
            context_terms.append(context['concept'])
            for item in context.get('related_items', []):
                context_terms.append(item['name'])
        
        enhanced_query = f"{query} {' '.join(context_terms[:10])}"  # Limit context terms
        
        # Embed the enhanced query
        question_embedding = client.multimodal_embed(
            inputs=[[enhanced_query]],
            model="voyage-multimodal-3",
            input_type="document"
        ).embeddings[0]
        
        # FAISS search with enhanced query
        faiss_results = []
        if self.faiss_index:
            query_embedding = np.array([question_embedding]).astype("float32")
            distances, indices = self.faiss_index.search(query_embedding, top_k)
            
            faiss_results = [
                {
                    "text": self.metadata[idx],
                    "score": float(1 / (1 + distances[0][i])),
                    "source": "FAISS"
                }
                for i, idx in enumerate(indices[0])
            ]
        
        # Neo4j vector search
        neo4j_results = []
        if self.neo4j_driver and self.vector_index_created:
            try:
                with self.neo4j_driver.session() as session:
                    result = session.run(
                        """
                        CALL db.index.vector.queryNodes('document_embeddings', $top_k, $embedding)
                        YIELD node, score
                        RETURN node.text AS text, score, node.source AS source
                        ORDER BY score DESC
                        """,
                        {"top_k": top_k, "embedding": question_embedding}
                    )
                    neo4j_results = [{
                        "text": record["text"],
                        "score": float(record["score"]),
                        "source": record["source"]
                    } for record in result]
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Neo4j search failed: {str(e)}")
        
        return {
            "faiss_results": faiss_results,
            "neo4j_results": neo4j_results,
            "graph_context": graph_context,
            "enhanced_query": enhanced_query
        }

    def query_llm_with_context(self, query: str, context: List[Dict], graph_context: List[Dict] = None, chat_history: List[Dict] = None) -> str:
        """Query Ollama LLM with GraphRAG context using chat interface"""
        try:
            # Prepare graph context information
            graph_info = ""
            if graph_context:
                graph_info = "\n### Knowledge Graph Context:\n"
                for ctx in graph_context[:3]:  # Limit to top 3
                    graph_info += f"- **{ctx['concept']}**: {ctx.get('description', 'No description')}\n"
                    if ctx.get('related_items'):
                        related = [item['name'] for item in ctx['related_items'][:3]]
                        graph_info += f"  Related: {', '.join(related)}\n"
            
            # Prepare document context
            context_text = ""
            if context:
                context_text = "\n### Document Context:\n"
                for i, res in enumerate(context[:3], 1):
                    context_text += f"{i}. [Relevance: {res['score']:.2f}]\n{res['text'][:500]}...\n\n"
            
            # Create system message
            system_message = f"""You are a helpful technical assistant with access to document content and a knowledge graph. 
Use the provided context and graph relationships to give comprehensive, accurate answers.
Be specific and reference the technical details from the context when relevant.

{graph_info}
{context_text}"""
            
            # Prepare chat messages with conversation history
            messages = [{"role": "system", "content": system_message}]
            
            # Add recent conversation history (last 6 messages)
            if chat_history:
                recent_history = chat_history[-6:]
                for msg in recent_history:
                    if msg["role"] in ["user", "assistant"]:
                        messages.append({
                            "role": msg["role"], 
                            "content": msg["content"]
                        })
            
            # Add current query
            messages.append({"role": "user", "content": query})
            
            # Generate response using chat interface
            llm_response = self.llm.chat_generate(
                messages,
                temperature=0.7,
                top_p=0.9,
                num_ctx=4096
            )
            
            return llm_response
            
        except Exception as e:
            logger.error(f"‚ö†Ô∏è LLM query failed: {str(e)}")
            return f"Error generating answer: {str(e)}"

    def simple_answer(self, query: str) -> str:
        """Generate a simple answer without RAG context"""
        try:
            prompt = f"""Answer this question clearly and concisely. If you don't have specific information, 
say so and provide general guidance if possible.

Question: {query}

Answer:"""
            
            return self.llm.simple_generate(prompt, temperature=0.7, num_ctx=2048)
        except Exception as e:
            return f"Error generating simple answer: {str(e)}"

    def ask_question(self, question: str, chat_history: List[Dict] = None) -> Tuple[str, int, int]:
        """Ask a question and get an answer with context info"""
        try:
            results = self.graphrag_search(question)
            context = results["faiss_results"] or results["neo4j_results"]
            graph_context = results["graph_context"]
            
            if context or graph_context:
                answer = self.query_llm_with_context(question, context, graph_context, chat_history)
                return answer, len(context), len(graph_context) if graph_context else 0
            else:
                answer = self.simple_answer(question)
                return answer, 0, 0
        except Exception as e:
            logger.error(f"Error in ask_question: {str(e)}")
            return f"Error: {str(e)}", 0, 0

    def close(self):
        """Clean up resources"""
        if self.neo4j_driver:
            self.neo4j_driver.close()
            logger.info("‚úÖ Neo4j connection closed")

def generate_chat_title(first_message: str) -> str:
    """Generate a chat title from the first message"""
    # Clean and truncate the message
    title = first_message.strip()
    title = re.sub(r'\s+', ' ', title)  # Replace multiple spaces with single space
    
    # Truncate to reasonable length
    if len(title) > 50:
        title = title[:47] + "..."
    
    return title if title else "New Chat"

@st.cache_resource
def initialize_system():
    """Initialize the GraphRAG system (cached)"""
    db_manager = DatabaseManager()
    system = GraphRAGSystem(db_manager)
    system.connect_neo4j()
    return system, db_manager

def load_document_if_needed(system: GraphRAGSystem):
    """Load and process PDF document if needed"""
    if not system.faiss_index and os.path.exists(PDF_PATH):
        with st.spinner("üìÇ Processing PDF document with GraphRAG..."):
            try:
                document = system.process_pdf(PDF_PATH)
                documents = [document]
                
                # Create FAISS index and extract knowledge
                embed_response, processed_docs = system.create_faiss_index(documents)
                
                # Store documents in Neo4j
                system.store_in_neo4j(processed_docs, embed_response.embeddings)
                
                st.success("‚úÖ Document processing complete!")
                return True
            except Exception as e:
                st.error(f"‚ùå Failed to process document: {str(e)}")
                return False
    return system.faiss_index is not None

def main():
    """Main Streamlit application"""
    st.title("ü§ñ GraphRAG Chat System")
    st.markdown("Chat with your documents using Knowledge Graph-enhanced RAG")
    
    # Initialize system
    try:
        system, db_manager = initialize_system()
    except Exception as e:
        st.error(f"‚ùå Failed to initialize system: {str(e)}")
        return
    
    # Sidebar for chat management
    with st.sidebar:
        st.header("üí¨ Chat Management")
        
        # New chat button
        if st.button("‚ûï New Chat", use_container_width=True):
            st.session_state.current_chat_id = None
            st.session_state.messages = []
            st.rerun()
        
        # Load existing chats
        chats = db_manager.get_chats()
        
        if chats:
            st.subheader("üìã Recent Chats")
            
            for chat in chats:
                chat_id = str(chat['id'])
                chat_title = chat['title']
                message_count = chat.get('message_count', 0)
                last_message_time = chat.get('last_message_time')
                
                # Format last message time
                time_str = ""
                if last_message_time:
                    try:
                        if isinstance(last_message_time, str):
                            last_time = datetime.fromisoformat(last_message_time.replace('Z', '+00:00'))
                        else:
                            last_time = last_message_time
                        
                        now = datetime.now(last_time.tzinfo) if last_time.tzinfo else datetime.now()
                        diff = now - last_time
                        
                        if diff.days > 0:
                            time_str = f"{diff.days}d ago"
                        elif diff.seconds > 3600:
                            time_str = f"{diff.seconds // 3600}h ago"
                        elif diff.seconds > 60:
                            time_str = f"{diff.seconds // 60}m ago"
                        else:
                            time_str = "Just now"
                    except:
                        time_str = ""
                
                # Chat item with selection
                col1, col2 = st.columns([4, 1])
                
                with col1:
                    if st.button(
                        f"üí¨ {chat_title}",
                        key=f"chat_{chat_id}",
                        help=f"{message_count} messages ‚Ä¢ {time_str}",
                        use_container_width=True
                    ):
                        st.session_state.current_chat_id = chat_id
                        # Load chat messages
                        messages = db_manager.get_chat_messages(chat_id)
                        st.session_state.messages = []
                        for msg in messages:
                            if msg['role'] in ['user', 'assistant']:
                                st.session_state.messages.append({
                                    "role": msg['role'],
                                    "content": msg['content']
                                })
                        st.rerun()
                
                with col2:
                    if st.button("üóëÔ∏è", key=f"delete_{chat_id}", help="Delete chat"):
                        if db_manager.delete_chat(chat_id):
                            if st.session_state.get('current_chat_id') == chat_id:
                                st.session_state.current_chat_id = None
                                st.session_state.messages = []
                            st.rerun()
        else:
            st.info("No chats yet. Start a new conversation!")
        
        # System status
        st.subheader("üîß System Status")
        doc_loaded = load_document_if_needed(system)
        
        status_items = [
            ("ü§ñ Ollama LLM", f"‚úÖ {system.llm.model}" if system.llm else "‚ùå Not connected"),
            ("üìä FAISS Index", "‚úÖ Loaded" if doc_loaded else "‚ùå Not loaded"),
            ("üóÑÔ∏è Neo4j", "‚úÖ Connected" if system.neo4j_driver else "‚ùå Not connected"),
            ("üíæ Database", "‚úÖ Connected" if db_manager.connection else "‚ùå Not connected"),
        ]
        
        for label, status in status_items:
            st.text(f"{label}: {status}")

    # Main chat interface
    
    # Initialize session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "current_chat_id" not in st.session_state:
        st.session_state.current_chat_id = None

    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Chat input
    if prompt := st.chat_input("Ask me anything about your document..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)

        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("ü§î Thinking..."):
                try:
                    # Create new chat if needed
                    if not st.session_state.current_chat_id:
                        chat_title = generate_chat_title(prompt)
                        chat_id = db_manager.create_chat(chat_title)
                        if chat_id:
                            st.session_state.current_chat_id = chat_id
                        else:
                            st.error("Failed to create new chat")
                            return
                    
                    # Get chat history for context
                    chat_history = []
                    if st.session_state.current_chat_id:
                        chat_messages = db_manager.get_chat_messages(st.session_state.current_chat_id)
                        chat_history = [{"role": msg["role"], "content": msg["content"]} for msg in chat_messages]
                    
                    # Get answer from GraphRAG system
                    answer, context_used, graph_context_used = system.ask_question(prompt, chat_history)
                    
                    # Display answer
                    st.markdown(answer)
                    
                    # Add to session state
                    st.session_state.messages.append({"role": "assistant", "content": answer})
                    
                    # Save to database
                    if st.session_state.current_chat_id:
                        # Save user message
                        db_manager.add_message(
                            st.session_state.current_chat_id,
                            "user",
                            prompt
                        )
                        
                        # Save assistant message
                        db_manager.add_message(
                            st.session_state.current_chat_id,
                            "assistant",
                            answer,
                            context_used=context_used,
                            graph_context_used=graph_context_used
                        )
                    
                    # Show context info
                    if context_used > 0 or graph_context_used > 0:
                        st.info(f"üìä Used {context_used} document chunks and {graph_context_used} knowledge graph concepts")
                        
                except Exception as e:
                    error_msg = f"‚ùå Error: {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})

    # Footer
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666;'>
            üöÄ GraphRAG Chat System | Powered by Voyage AI, Ollama, Neo4j & Supabase
        </div>
        """, 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()

import voyageai
import faiss
import numpy as np
import fitz  # PyMuPDF
from PIL import Image
import io
import requests
from neo4j import GraphDatabase
from typing import List, Dict, Union, Optional, Tuple
import os
import json
import re
from datetime import datetime, timedelta
import logging

# Configuration
VOYAGE_API_KEY = "pa-tDh9PAJmIfaPahq1-GkuSk8uVNGrI69sq3uxpiGK8Y7"
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_CHAT_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "llama3.2:1b"
PDF_PATH = r"C:\Users\STW\Downloads\MA_DWM1000_2000_en_120509.pdf"
CONTEXT_FILE = "search_context.json"

# Neo4j Connection
NEO4J_URI = "bolt://localhost:7687"
NEO4J_AUTH = ("neo4j", "omarnasser")

# Initialize Voyage AI client
voyageai.api_key = VOYAGE_API_KEY
client = voyageai.Client()

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OllamaLLM:
    """Dedicated Ollama LLM handler with improved error handling and features"""
    
    def __init__(self, base_url: str = OLLAMA_URL, model: str = OLLAMA_MODEL):
        self.base_url = base_url
        self.chat_url = OLLAMA_CHAT_URL
        self.model = model
        self.check_ollama_connection()
    
    def check_ollama_connection(self):
        """Check if Ollama is running and model is available"""
        try:
            # Check if Ollama is running
            response = requests.get(f"{self.base_url.replace('/api/generate', '')}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                
                if self.model not in model_names:
                    logger.warning(f"‚ö†Ô∏è Model '{self.model}' not found. Available models: {model_names}")
                    if model_names:
                        self.model = model_names[0]
                        logger.info(f"üîÑ Switching to available model: {self.model}")
                else:
                    logger.info(f"‚úÖ Ollama connected with model: {self.model}")
            else:
                logger.error("‚ùå Ollama server not responding")
        except Exception as e:
            logger.error(f"‚ùå Failed to connect to Ollama: {str(e)}")
            logger.info("üí° Make sure Ollama is running: 'ollama serve'")
    
    def simple_generate(self, prompt: str, **kwargs) -> str:
        """Simple text generation using /api/generate endpoint"""
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": kwargs.get("temperature", 0.3),
                    "top_p": kwargs.get("top_p", 0.9),
                    "num_ctx": kwargs.get("num_ctx", 2048),
                    "repeat_penalty": kwargs.get("repeat_penalty", 1.1)
                }
            }
            
            response = requests.post(self.base_url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "No response from LLM")
            
        except requests.exceptions.Timeout:
            return "Error: LLM request timed out"
        except requests.exceptions.RequestException as e:
            return f"Error: LLM request failed - {str(e)}"
        except Exception as e:
            return f"Error: {str(e)}"
    
    def chat_generate(self, messages: List[Dict], **kwargs) -> str:
        """Chat-based generation using /api/chat endpoint"""
        try:
            payload = {
                "model": self.model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "top_p": kwargs.get("top_p", 0.9),
                    "num_ctx": kwargs.get("num_ctx", 4096),
                    "repeat_penalty": kwargs.get("repeat_penalty", 1.1)
                }
            }
            
            response = requests.post(self.chat_url, json=payload, timeout=90)
            response.raise_for_status()
            
            result = response.json()
            return result.get("message", {}).get("content", "No response from LLM")
            
        except requests.exceptions.Timeout:
            return "Error: Chat request timed out"
        except requests.exceptions.RequestException as e:
            return f"Error: Chat request failed - {str(e)}"
        except Exception as e:
            return f"Error: {str(e)}"

class GraphRAGSystem:
    def __init__(self):
        self.faiss_index = None
        self.metadata = []
        self.neo4j_driver = None
        self.vector_index_created = False
        self.knowledge_graph_created = False
        self.conversation_history = []
        self.current_session_id = None
        self.llm = OllamaLLM()  # Initialize Ollama LLM
        self.load_context()
        
    def load_context(self):
        """Load previous context from file if exists"""
        try:
            if os.path.exists(CONTEXT_FILE):
                with open(CONTEXT_FILE, 'r') as f:
                    data = json.load(f)
                    self.conversation_history = data.get('conversation_history', [])
                    self.current_session_id = data.get('current_session_id', self.generate_session_id())
                    logger.info("‚úÖ Loaded previous conversation context")
            else:
                self.current_session_id = self.generate_session_id()
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to load context: {str(e)}")
            self.current_session_id = self.generate_session_id()
    
    def generate_session_id(self):
        """Generate unique session ID"""
        return f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    def save_context(self):
        """Save current context to file"""
        try:
            with open(CONTEXT_FILE, 'w') as f:
                json.dump({
                    'conversation_history': self.conversation_history,
                    'current_session_id': self.current_session_id,
                    'last_updated': datetime.now().isoformat()
                }, f, indent=2)
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to save context: {str(e)}")
    
    def connect_neo4j(self):
        """Connect to Neo4j database and setup GraphRAG schema"""
        try:
            self.neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=NEO4J_AUTH)
            
            # Verify connection
            with self.neo4j_driver.session() as session:
                result = session.run("RETURN 1 AS test")
                if result.single()["test"] == 1:
                    logger.info("‚úÖ Successfully connected to Neo4j")
                    
            # Create vector index and knowledge graph schema
            self.create_neo4j_vector_index()
            self.create_knowledge_graph_schema()
            
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Neo4j connection failed: {str(e)}")
            self.neo4j_driver = None

    def create_neo4j_vector_index(self):
        """Create vector index in Neo4j"""
        if not self.neo4j_driver:
            return
            
        try:
            with self.neo4j_driver.session() as session:
                # Check if vector index exists
                index_exists = session.run(
                    "SHOW INDEXES WHERE type = 'VECTOR' AND name = 'document_embeddings'"
                ).single()
                
                if not index_exists:
                    session.run("""
                    CREATE VECTOR INDEX document_embeddings 
                    FOR (d:Document) ON (d.embedding)
                    OPTIONS {
                        indexConfig: {
                            `vector.dimensions`: 1024,
                            `vector.similarity_function`: 'cosine'
                        }
                    }
                    """)
                    logger.info("‚úÖ Created vector index in Neo4j")
                    
                self.vector_index_created = True
                    
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to create vector index: {str(e)}")
            self.vector_index_created = False

    def create_knowledge_graph_schema(self):
        """Create GraphRAG knowledge graph schema"""
        if not self.neo4j_driver:
            return
            
        try:
            with self.neo4j_driver.session() as session:
                # Create constraints and indexes for GraphRAG
                session.run("""
                CREATE CONSTRAINT unique_concept IF NOT EXISTS 
                FOR (c:Concept) REQUIRE c.name IS UNIQUE
                """)
                
                session.run("""
                CREATE CONSTRAINT unique_entity IF NOT EXISTS 
                FOR (e:Entity) REQUIRE e.name IS UNIQUE
                """)
                
                session.run("""
                CREATE CONSTRAINT unique_session IF NOT EXISTS 
                FOR (s:Session) REQUIRE s.id IS UNIQUE
                """)
                
                session.run("""
                CREATE CONSTRAINT unique_query IF NOT EXISTS 
                FOR (q:Query) REQUIRE q.id IS UNIQUE
                """)
                
                # Create fulltext search index
                try:
                    session.run("""
                    CREATE FULLTEXT INDEX conceptSearch IF NOT EXISTS 
                    FOR (n:Concept|Entity) ON EACH [n.name, n.description]
                    """)
                except:
                    pass  # Index might already exist
                
                # Create vector index for concept embeddings
                try:
                    session.run("""
                    CREATE VECTOR INDEX concept_embeddings IF NOT EXISTS
                    FOR (c:Concept) ON (c.embedding)
                    OPTIONS {
                        indexConfig: {
                            `vector.dimensions`: 1024,
                            `vector.similarity_function`: 'cosine'
                        }
                    }
                    """)
                except:
                    pass  # Index might already exist
                
                logger.info("‚úÖ Created GraphRAG knowledge graph schema")
                self.knowledge_graph_created = True
                
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to create knowledge graph schema: {str(e)}")
            self.knowledge_graph_created = False

    def extract_entities_and_concepts(self, text: str) -> Dict:
        """Extract entities and concepts using Ollama LLM"""
        prompt = f"""Extract key concepts, entities, and their relationships from this technical text. 
Focus on technical terms, specifications, product names, and important concepts.

Text: {text[:2000]}...

Return ONLY a valid JSON object with this exact structure:
{{
    "concepts": [
        {{"name": "concept_name", "description": "brief description", "importance": 0.8, "type": "technical"}}
    ],
    "entities": [
        {{"name": "entity_name", "description": "brief description", "type": "product"}}
    ],
    "relationships": [
        {{"source": "source_name", "target": "target_name", "type": "relates_to", "strength": 0.7}}
    ]
}}

Ensure all JSON is properly formatted and valid."""
        
        try:
            response = self.llm.simple_generate(prompt, temperature=0.1, num_ctx=3000)
            
            # Clean the response to extract JSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                parsed_data = json.loads(json_str)
                
                # Ensure required fields exist
                parsed_data.setdefault('concepts', [])
                parsed_data.setdefault('entities', [])
                parsed_data.setdefault('relationships', [])
                
                return parsed_data
            else:
                logger.warning("No valid JSON found in LLM response")
                return {"concepts": [], "entities": [], "relationships": []}
                
        except json.JSONDecodeError as e:
            logger.error(f"JSON decode error: {str(e)}")
            return {"concepts": [], "entities": [], "relationships": []}
        except Exception as e:
            logger.error(f"Failed to extract entities: {str(e)}")
            return {"concepts": [], "entities": [], "relationships": []}

    def store_knowledge_graph(self, knowledge: Dict, source_text: str):
        """Store extracted knowledge in Neo4j graph"""
        if not self.neo4j_driver or not self.knowledge_graph_created:
            return
            
        try:
            with self.neo4j_driver.session() as session:
                # Store concepts with embeddings
                for concept in knowledge.get('concepts', []):
                    try:
                        # Generate embedding for concept
                        concept_text = concept['name'] + " " + concept.get('description', '')
                        concept_embedding = client.multimodal_embed(
                            inputs=[[concept_text]],
                            model="voyage-multimodal-3",
                            input_type="document"
                        ).embeddings[0]
                        
                        session.run("""
                        MERGE (c:Concept {name: $name})
                        SET c.description = $description,
                            c.importance = $importance,
                            c.type = $type,
                            c.embedding = $embedding,
                            c.last_mentioned = datetime(),
                            c.mention_count = coalesce(c.mention_count, 0) + 1
                        """, {
                            **concept,
                            'embedding': concept_embedding
                        })
                    except Exception as e:
                        logger.error(f"Failed to store concept {concept.get('name', 'unknown')}: {str(e)}")
                        continue
                
                # Store entities
                for entity in knowledge.get('entities', []):
                    session.run("""
                    MERGE (e:Entity {name: $name})
                    SET e.description = $description,
                        e.type = $type,
                        e.last_mentioned = datetime(),
                        e.mention_count = coalesce(e.mention_count, 0) + 1
                    """, entity)
                
                # Store relationships
                for rel in knowledge.get('relationships', []):
                    session.run("""
                    MATCH (source) WHERE source.name = $source
                    MATCH (target) WHERE target.name = $target
                    MERGE (source)-[r:RELATES {type: $type}]->(target)
                    SET r.strength = $strength,
                        r.last_used = datetime(),
                        r.usage_count = coalesce(r.usage_count, 0) + 1
                    """, rel)
                
                # Create session and link to current conversation
                session.run("""
                MERGE (s:Session {id: $session_id})
                SET s.last_active = datetime()
                """, {"session_id": self.current_session_id})
                
                logger.info(f"‚úÖ Stored knowledge graph: {len(knowledge.get('concepts', []))} concepts, {len(knowledge.get('entities', []))} entities")
                
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to store knowledge graph: {str(e)}")

    def get_graph_context(self, query: str, top_k: int = 5) -> List[Dict]:
        """Retrieve relevant context from knowledge graph"""
        if not self.neo4j_driver or not self.knowledge_graph_created:
            return []
            
        try:
            # Generate query embedding
            query_embedding = client.multimodal_embed(
                inputs=[[query]],
                model="voyage-multimodal-3",
                input_type="document"
            ).embeddings[0]
            
            with self.neo4j_driver.session() as session:
                # Vector similarity search on concepts
                vector_results = []
                try:
                    vector_results = session.run("""
                    CALL db.index.vector.queryNodes('concept_embeddings', $top_k, $embedding)
                    YIELD node, score
                    MATCH (node)-[r:RELATES]-(related)
                    RETURN node.name as concept, 
                           node.description as description,
                           score,
                           collect(DISTINCT {name: related.name, type: labels(related)[0], relationship: r.type}) as related_items
                    ORDER BY score DESC
                    """, {"top_k": top_k, "embedding": query_embedding}).data()
                except Exception as e:
                    logger.warning(f"Vector search failed: {str(e)}")
                
                # Also try fulltext search as fallback
                fulltext_results = []
                try:
                    fulltext_results = session.run("""
                    CALL db.index.fulltext.queryNodes('conceptSearch', $query)
                    YIELD node, score
                    WHERE score > 0.5
                    MATCH (node)-[r:RELATES]-(related)
                    RETURN node.name as concept, 
                           node.description as description,
                           score,
                           collect(DISTINCT {name: related.name, type: labels(related)[0], relationship: r.type}) as related_items
                    ORDER BY score DESC
                    LIMIT $top_k
                    """, {"query": query, "top_k": top_k}).data()
                except Exception as e:
                    logger.warning(f"Fulltext search failed: {str(e)}")
                
                # Combine and deduplicate results
                all_results = vector_results + fulltext_results
                seen_concepts = set()
                unique_results = []
                
                for result in all_results:
                    if result['concept'] not in seen_concepts:
                        seen_concepts.add(result['concept'])
                        unique_results.append(result)
                
                return unique_results[:top_k]
                
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to get graph context: {str(e)}")
            return []

    def process_pdf(self, pdf_path: str) -> List[Union[str, Image.Image]]:
        """Extract text and images from PDF"""
        doc = fitz.open(pdf_path)
        pdf_text = ""
        pdf_images = []

        for page in doc:
            # Extract text
            pdf_text += page.get_text() + "\n"
            
            # Extract images (limit to avoid too many)
            image_list = page.get_images(full=True)
            for i, img in enumerate(image_list):
                if len(pdf_images) >= 5:  # Limit images
                    break
                try:
                    xref = img[0]
                    base_image = doc.extract_image(xref)
                    image_bytes = base_image["image"]
                    pdf_images.append(Image.open(io.BytesIO(image_bytes)).resize((256, 256)))
                except:
                    continue
        
        doc.close()
        return [pdf_text, *pdf_images] if pdf_images else [pdf_text]

    def chunk_document(self, text: str, chunk_size: int = 1000) -> List[str]:
        """Split document into manageable chunks"""
        words = text.split()
        chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
        return chunks

    def create_faiss_index(self, documents: List[List[Union[str, Image.Image]]]):
        """Create FAISS index and extract knowledge from documents"""
        # Process documents into chunks
        processed_docs = []
        all_text = ""
        
        for doc in documents:
            text = doc[0]
            images = doc[1:] if len(doc) > 1 else []
            all_text += text + " "
            
            # Split text into chunks
            chunks = self.chunk_document(text)
            for chunk in chunks:
                # Only add images to first few chunks to avoid too many
                if len(processed_docs) < 3 and images:
                    processed_docs.append([chunk, *images[:2]])  # Max 2 images per chunk
                else:
                    processed_docs.append([chunk])
        
        logger.info(f"üîÑ Processing {len(processed_docs)} document chunks...")
        
        # Extract and store knowledge from the entire document
        logger.info("üß† Extracting knowledge graph from document...")
        knowledge = self.extract_entities_and_concepts(all_text)
        self.store_knowledge_graph(knowledge, all_text)
        
        # Get embeddings for all chunks
        response = client.multimodal_embed(
            inputs=processed_docs,
            model="voyage-multimodal-3",
            input_type="document"
        )
        
        embeddings = np.array(response.embeddings).astype("float32")
        dimension = embeddings.shape[1]
        
        # Create and populate FAISS index
        self.faiss_index = faiss.IndexFlatL2(dimension)
        self.faiss_index.add(embeddings)
        self.metadata = [doc[0] for doc in processed_docs]
        
        logger.info(f"‚úÖ Created FAISS index with {len(embeddings)} vectors")
        return response, processed_docs

    def store_in_neo4j(self, documents: List[List[Union[str, Image.Image]]], embeddings: List[List[float]]):
        """Store documents and embeddings in Neo4j"""
        if not self.neo4j_driver or not self.vector_index_created:
            logger.warning("‚ö†Ô∏è Neo4j not properly configured - skipping document storage")
            return
            
        try:
            with self.neo4j_driver.session() as session:
                for i, (doc, embedding) in enumerate(zip(documents, embeddings)):
                    session.run(
                        """
                        MERGE (d:Document {id: $id})
                        SET d.text = $text,
                            d.embedding = $embedding,
                            d.has_image = $has_image,
                            d.source = $source,
                            d.chunk_index = $chunk_index,
                            d.timestamp = datetime()
                        """,
                        {
                            "id": f"doc_{i}",
                            "text": doc[0],
                            "embedding": embedding,
                            "has_image": len(doc) > 1,
                            "source": PDF_PATH,
                            "chunk_index": i
                        }
                    )
            logger.info(f"üíæ Stored {len(documents)} document chunks in Neo4j")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to store documents in Neo4j: {str(e)}")

    def graphrag_search(self, query: str, top_k: int = 3) -> Dict:
        """Enhanced search using GraphRAG approach"""
        # Store query in conversation history
        self.conversation_history.append({
            "role": "user",
            "content": query,
            "timestamp": datetime.now().isoformat()
        })
        
        # Extract knowledge from query and store in graph
        query_knowledge = self.extract_entities_and_concepts(query)
        self.store_knowledge_graph(query_knowledge, query)
        
        # Get graph context
        graph_context = self.get_graph_context(query, top_k)
        
        # Enhance query with graph context
        context_terms = []
        for context in graph_context:
            context_terms.append(context['concept'])
            for item in context.get('related_items', []):
                context_terms.append(item['name'])
        
        enhanced_query = f"{query} {' '.join(context_terms[:10])}"  # Limit context terms
        
        # Embed the enhanced query
        question_embedding = client.multimodal_embed(
            inputs=[[enhanced_query]],
            model="voyage-multimodal-3",
            input_type="document"
        ).embeddings[0]
        
        # FAISS search with enhanced query
        faiss_results = []
        if self.faiss_index:
            query_embedding = np.array([question_embedding]).astype("float32")
            distances, indices = self.faiss_index.search(query_embedding, top_k)
            
            faiss_results = [
                {
                    "text": self.metadata[idx],
                    "score": float(1 / (1 + distances[0][i])),
                    "source": "FAISS"
                }
                for i, idx in enumerate(indices[0])
            ]
        
        # Neo4j vector search
        neo4j_results = []
        if self.neo4j_driver and self.vector_index_created:
            try:
                with self.neo4j_driver.session() as session:
                    result = session.run(
                        """
                        CALL db.index.vector.queryNodes('document_embeddings', $top_k, $embedding)
                        YIELD node, score
                        RETURN node.text AS text, score, node.source AS source
                        ORDER BY score DESC
                        """,
                        {"top_k": top_k, "embedding": question_embedding}
                    )
                    neo4j_results = [{
                        "text": record["text"],
                        "score": float(record["score"]),
                        "source": record["source"]
                    } for record in result]
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Neo4j search failed: {str(e)}")
        
        return {
            "faiss_results": faiss_results,
            "neo4j_results": neo4j_results,
            "graph_context": graph_context,
            "enhanced_query": enhanced_query
        }

    def query_llm_with_context(self, query: str, context: List[Dict], graph_context: List[Dict] = None) -> str:
        """Query Ollama LLM with GraphRAG context using chat interface"""
        try:
            # Prepare graph context information
            graph_info = ""
            if graph_context:
                graph_info = "\n### Knowledge Graph Context:\n"
                for ctx in graph_context[:3]:  # Limit to top 3
                    graph_info += f"- **{ctx['concept']}**: {ctx.get('description', 'No description')}\n"
                    if ctx.get('related_items'):
                        related = [item['name'] for item in ctx['related_items'][:3]]
                        graph_info += f"  Related: {', '.join(related)}\n"
            
            # Prepare document context
            context_text = ""
            if context:
                context_text = "\n### Document Context:\n"
                for i, res in enumerate(context[:3], 1):
                    context_text += f"{i}. [Relevance: {res['score']:.2f}]\n{res['text'][:500]}...\n\n"
            
            # Create system message
            system_message = f"""You are a helpful technical assistant with access to document content and a knowledge graph. 
Use the provided context and graph relationships to give comprehensive, accurate answers.
Be specific and reference the technical details from the context when relevant.

{graph_info}
{context_text}"""
            
            # Prepare chat messages with conversation history
            messages = [{"role": "system", "content": system_message}]
            
            # Add recent conversation history (last 6 messages)
            recent_history = self.conversation_history[-6:]
            for msg in recent_history:
                if msg["role"] in ["user", "assistant"]:
                    messages.append({
                        "role": msg["role"], 
                        "content": msg["content"]
                    })
            
            # Add current query
            messages.append({"role": "user", "content": query})
            
            # Generate response using chat interface
            llm_response = self.llm.chat_generate(
                messages,
                temperature=0.7,
                top_p=0.9,
                num_ctx=4096
            )
            
            # Store response in conversation history
            self.conversation_history.append({
                "role": "assistant",
                "content": llm_response,
                "timestamp": datetime.now().isoformat(),
                "context_used": len(context),
                "graph_context_used": len(graph_context) if graph_context else 0
            })
            
            self.save_context()
            return llm_response
            
        except Exception as e:
            logger.error(f"‚ö†Ô∏è LLM query failed: {str(e)}")
            return f"Error generating answer: {str(e)}"

    def simple_answer(self, query: str) -> str:
        """Generate a simple answer without RAG context"""
        try:
            prompt = f"""Answer this question clearly and concisely. If you don't have specific information, 
say so and provide general guidance if possible.

Question: {query}

Answer:"""
            
            return self.llm.simple_generate(prompt, temperature=0.7, num_ctx=2048)
        except Exception as e:
            return f"Error generating simple answer: {str(e)}"

    def interactive_search(self):
        """Interactive GraphRAG search interface"""
        print("\nüöÄ GraphRAG System with Ollama - Interactive Mode")
        print("Commands: 'exit' to quit, 'clear' to reset conversation, 'graph' to show graph stats")
        print("GraphRAG features: Knowledge graph context, entity extraction, relationship mapping\n")
        print(f"ü§ñ Using Ollama model: {self.llm.model}")
        print(f"üìä FAISS index loaded: {'‚úÖ' if self.faiss_index else '‚ùå'}")
        print(f"üóÑÔ∏è Neo4j connected: {'‚úÖ' if self.neo4j_driver else '‚ùå'}")
        
        while True:
            try:
                query = input("\n‚ùì Your question: ").strip()
                
                if query.lower() == 'exit':
                    print("üëã Goodbye!")
                    break
                    
                if query.lower() == 'clear':
                    self.conversation_history = []
                    self.save_context()
                    print("üßπ Conversation history cleared")
                    continue
                
                if query.lower() == 'graph':
                    self.show_graph_stats()
                    continue
                
                if not query:
                    continue
                
                # Perform GraphRAG search
                print("\nüîé Searching with GraphRAG...")
                results = self.graphrag_search(query)
                
                # Combine results (prefer FAISS results if available)
                context = results["faiss_results"] or results["neo4j_results"]
                graph_context = results["graph_context"]
                
                if context or graph_context:
                    # Generate answer with context
                    answer = self.query_llm_with_context(query, context, graph_context)
                    
                    print("\nüí° GraphRAG Answer:")
                    print(answer.strip())
                    
                    # Show sources and graph context
                    if context:
                        print("\nüìö Document Sources:")
                        for i, res in enumerate(context[:3], 1):
                            source_preview = res['text'][:100].replace('\n', ' ')
                            print(f"{i}. [Relevance: {res['score']:.2f}] {source_preview}...")
                    
                    if graph_context:
                        print(f"\nüß† Knowledge Graph Context ({len(graph_context)} concepts):")
                        for ctx in graph_context[:3]:
                            related_count = len(ctx.get('related_items', []))
                            print(f"‚Ä¢ {ctx['concept']} (similarity: {ctx.get('score', 0):.2f}, {related_count} connections)")
                    
                else:
                    print("‚ö†Ô∏è No specific context found, generating general answer...")
                    answer = self.simple_answer(query)
                    print("\nüí° General Answer:")
                    print(answer.strip())
                    
            except KeyboardInterrupt:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                logger.error(f"‚ö†Ô∏è An error occurred: {str(e)}")
                print(f"‚ö†Ô∏è An error occurred: {str(e)}")

    def show_graph_stats(self):
        """Show knowledge graph statistics"""
        if not self.neo4j_driver:
            print("‚ö†Ô∏è Neo4j not connected")
            return
            
        try:
            with self.neo4j_driver.session() as session:
                # Get counts
                concepts = session.run("MATCH (c:Concept) RETURN count(c) as count").single()["count"]
                entities = session.run("MATCH (e:Entity) RETURN count(e) as count").single()["count"]
                relationships = session.run("MATCH ()-[r:RELATES]->() RETURN count(r) as count").single()["count"]
                documents = session.run("MATCH (d:Document) RETURN count(d) as count").single()["count"]
                
                print(f"\nüìä Knowledge Graph Statistics:")
                print(f"‚Ä¢ Concepts: {concepts}")
                print(f"‚Ä¢ Entities: {entities}")
                print(f"‚Ä¢ Relationships: {relationships}")
                print(f"‚Ä¢ Documents: {documents}")
                print(f"‚Ä¢ Conversation history: {len(self.conversation_history)} messages")
                
                # Show top concepts
                top_concepts = session.run("""
                MATCH (c:Concept)
                RETURN c.name as name, c.mention_count as mentions
                ORDER BY c.mention_count DESC
                LIMIT 5
                """).data()
                
                if top_concepts:
                    print(f"\nüîù Top Concepts:")
                    for concept in top_concepts:
                        mentions = concept.get('mentions', 0)
                        print(f"‚Ä¢ {concept['name']} ({mentions} mentions)")
                
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Failed to get graph stats: {str(e)}")
            print(f"‚ö†Ô∏è Failed to get graph stats: {str(e)}")

    def ask_question(self, question: str) -> str:
        """Programmatic way to ask a question and get an answer"""
        try:
            results = self.graphrag_search(question)
            context = results["faiss_results"] or results["neo4j_results"]
            graph_context = results["graph_context"]
            
            if context or graph_context:
                return self.query_llm_with_context(question, context, graph_context)
            else:
                return self.simple_answer(question)
        except Exception as e:
            logger.error(f"Error in ask_question: {str(e)}")
            return f"Error: {str(e)}"

    def close(self):
        """Clean up resources"""
        if self.neo4j_driver:
            self.neo4j_driver.close()
            logger.info("‚úÖ Neo4j connection closed")

def main():
    """Main function to run the GraphRAG system"""
    search_system = None
    try:
        search_system = GraphRAGSystem()
        search_system.connect_neo4j()
        
        # Process PDF only if not already indexed
        if not search_system.faiss_index:
            logger.info("üìÇ Processing PDF document with GraphRAG...")
            if not os.path.exists(PDF_PATH):
                logger.error(f"‚ùå PDF file not found: {PDF_PATH}")
                print(f"‚ùå PDF file not found: {PDF_PATH}")
                print("Please update the PDF_PATH variable with the correct path to your PDF file.")
                return
            
            document = search_system.process_pdf(PDF_PATH)
            documents = [document]
            
            # Create FAISS index and extract knowledge
            embed_response, processed_docs = search_system.create_faiss_index(documents)
            
            # Store documents in Neo4j
            search_system.store_in_neo4j(processed_docs, embed_response.embeddings)
            
            logger.info("‚úÖ Document processing complete!")
        else:
            logger.info("‚úÖ Using existing document index")
        
        # Start interactive GraphRAG session
        search_system.interactive_search()
        
    except KeyboardInterrupt:
        print("\nüëã Shutting down gracefully...")
    except Exception as e:
        logger.error(f"Fatal error: {str(e)}")
        print(f"‚ùå Fatal error: {str(e)}")
        
        # Check common issues
        if "Connection refused" in str(e):
            print("\nüí° Troubleshooting tips:")
            print("1. Make sure Ollama is running: 'ollama serve'")
            print("2. Check if Neo4j is running (if using)")
            print("3. Verify your API keys and configurations")
    finally:
        if search_system:
            search_system.close()

def example_usage():
    """Example of how to use the system programmatically"""
    system = GraphRAGSystem()
    system.connect_neo4j()
    
    # Process document if needed
    if not system.faiss_index and os.path.exists(PDF_PATH):
        document = system.process_pdf(PDF_PATH)
        documents = [document]
        embed_response, processed_docs = system.create_faiss_index(documents)
        system.store_in_neo4j(processed_docs, embed_response.embeddings)
    
    # Ask questions programmatically
    questions = [
        "What is the operating voltage of the DWM2000 module?",
        "What are the key specifications?",
        "How does the module communicate?"
    ]
    
    for question in questions:
        print(f"\n‚ùì Question: {question}")
        answer = system.ask_question(question)
        print(f"üí° Answer: {answer}")
    
    system.close()

if __name__ == "__main__":
    try:
        import faiss
        
        # Check if running in interactive mode or example mode
        import sys
        if len(sys.argv) > 1 and sys.argv[1] == "example":
            example_usage()
        else:
            main()
            
    except ImportError:
        print("‚ùå Error: FAISS not installed. Install with:")
        print("conda install -c conda-forge faiss-cpu")
        print("or")
        print("pip install faiss-cpu")
    except Exception as e:
        logger.error(f"Fatal error: {str(e)}")
        print(f"‚ùå Fatal error: {str(e)}")
